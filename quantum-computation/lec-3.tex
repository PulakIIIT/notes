\chapter{Tensor product states}

\section{Postulates of QM}
\begin{itemize}
\item Associated to any isolated physical system is a complex vector space
with inner product. This space is called as the state space of the system.
This system is completely described by its state vector which is a unit
vector in the state space.
\end{itemize}

\section{Tensor product}

Let $A$ and $B$ be vector spaces with bases $A_{basis}, B_{basis}$.
$A \tensor B$ is a \emph{new vector space}, whose basis vectors are $a_i \tensor b_j$
where $a_i \in A_{basis}, b_i \in B_{basis}$.

Properties of the tensor product:
\begin{itemize}
    \item For any arbitrary scalar $z$ and element $v \in H_a$, $w \in H_b$,
        $z (\ket v \tensor \ket w) = (z \ket v) \tensor \ket w = \ket v \tensor (z \ket w)$
    \item $(\ket v_1 + \ket v_2) \tensor \ket w = \ket v_1 \tensor \ket w + \ket v_2 \tensor \ket w$
    \item $\ket w \tensor (\ket v_1 + \ket v_2)= \ket w \tensor \ket v_1 + \ket w \tensor \ket v_2$
    \textbf{TODO: what is an easy way to get correctly sized brackets?}
    \item Suppose $\ket v \in H_a, \ket w \in H_b$, and $A$ and $B$ are linear
        operators on $H_a$ and $H_b$ respectively. 
        $(A \tensor B) (\ket v \tensor \ket w) \equiv (A \ket v) \tensor (B \ket w)$.

    \item Let $C = \sum_i c_i A_i \tensor B_i$, where $A_i, B_i$ are linear
        operators on $H_a, H_b$. Now, $C (\ket v \tensor \ket w) = \sum_i c_i ((A_i \ket v) \tensor (B_i \ket w))$
    \item $\ket x = \sum_i a_i \ket v_i \tensor \ket w_i$. $\ket y = \sum_j b_j \ket v_j \tensor \ket w_j$.
        Now, $\bra{x}\ket{y} = (\sum_i a_i^* \bra v_i \tensor \bra w_i)(\sum_j b_j \ket v_j \tensor \ket w_j)$,
        which is equal to $\sum_i \sum_i a_i^* b_j \bra{v_i}\ket{v_j'} \bra{w_i}\ket{w_j'}$
\end{itemize}

This is way too redundant, \textbf{TODO:} write down the slick definition of tensor
product spaces seen in John Lee's intro to smooth manifolds, or the definition
seen in Tensor Geometry: The Geometric Viewpoint and its uses.

\begin{align*}
    \tr(A \ket \psi \bra \psi) = 
    \sum_i \bra i A \ket \psi \bra{\psi}\ket{i} =  
    \sum_i (\bra{\psi}\ket{i}) \cdot (\bra i A \ket \psi) = 
    \sum_i \bra{\psi}(\ket{i} \bra i) A \ket \psi = 
    \bra{\psi} A \ket \psi 
\end{align*}


\begin{theorem}
    Two operators $A$, $B$ are simeltanelously diagonalizable iff $[A, B] = 0$,
    where $[A, B] = AB - BA$. That is, there exists a basis where both $A$
    and $B$ are diagonal matrices.
\end{theorem}
\begin{proof}
    One direction of the proof is easy. If two operators are simeltanelously
    diagonalizable, then we can simply write both operators in this common
    basis. Diagonal matrices commute, hence $[A, B] = 0$.

    Let $\ket{a, j}$ be an orthonormal basis for the eigenspace $V_a$ of $A$
    with eigenvalue $a$ and index $j$ to label repeated eigenvalues.

    $AB \ket{a, j} = BA \ket{a, j} = a B \ket {a, j}$. Hence,
    $A (B \ket {a, j} = a (B \ket {a, j}$. Hence, $B \ket {a, j}$ is an
    eigenvector of $A$. Therefore, $B\ket{a, j} \in V_a$. 

    Define projector $P_a$ onto $V_a$. Now, define $B_a = P_a B P_a$.
\end{proof}
