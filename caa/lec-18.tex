\chapter{Applications of Tail Inequalities - 2}


\section{Polynomial verification}
check that $P_1(x) P_2(x) =_? P_3(x)$

If both polynomials have degree $n$, we can make it work in $n \log n$ using
FFT. We will design an algorithm faster than this.


\begin{itemize}
    \item Let $S \subset F$ be a subset of size at least $2n + 1$.
    \item We evaluate $P_1(s) P_2(s)$, and $P_3(s)$ for $s \in S$, s chosen uniformly
    at random (using Horner's method, this is $O(n)$ per point). The evaluations
    are the fingerprintts.
    
    \item Clearly, if $P_3(x) = P_1(x) P_2(x)$, this item will not make a
    mistake. This algorithm \textit{makes a mistake} if $P_3(x) \neq P_1(x) P_2(x)$, but the
    points we have in $S$ fail to catch this.
    \item The probability that this makes a mistake: We create a new polynomial
    $$Q(x) \equiv P_3(x) - P_1(x) P_2(x)$$
    
    It's degree is at most $2n$. If $P_3(x) \neq P_1(x) P_2(x)$, then $Q(x)$ is a
    nonzero polynomial.

    \item The polynomial $Q(x)$ has at most $2n$ roots. So, The probability
    that $Q(r) = 0$ has probability $2n/|S|$, which is the probability of the
    error.

    \item We can make the error rate polynomially small in $n$ by using
    repeated trials, or by picking a larger $S$.
\end{itemize}

This technique is useful when we don't have the polynomial directly available.
For example, maybe we are only given oracle access to evaluation. For example,
\textbf{permenant of a matrix}, apparently.


\section{Definitions to classify the kinds of error}

For both the algorithms considered, when the inputs are identical, the
algorithm does not make an error.

In inputs that are not identical, we make an error that is bounded by a
constant.

\subsection{The class $\rp$}
$\rp$ is the class of languages $L$ such that there exists a randomized
algorithm $A$ running in \textbf{worst case polynomial time}, such that for any input
$x$:

\begin{align*}
x \in L \implies \Pr(\text{$A$ accepts $x$}) \geq \frac{1}{2} \\
x \notin L \implies \Pr(\text{$A$ accepts $x$}) = 0
\end{align*}

The example was the \ip proof for graph non-isomorphism.


\subsection{The class $\corp$}
$\corp$ is the class of languages $L$ such that there exists a randomized
algorithm $A$ running in \textbf{worst case polynomial time}, such that for any input
$x$:

\begin{align*}
x \in L \implies \Pr(\text{$A$ accepts $x$})  = 0 \\
x \notin L \implies \Pr(\text{$A$ accepts $x$}) \leq \frac{1}{2}
\end{align*}


\subsection{Reflection on \rp and \corp}

The algorithms that we studed are the complement. We make no error
on strings in the language, but we can have an error on strings that are 
not in the language. So, the algorithms we studied are \corp.

These are considered Monte-Carlo algorithms.


\subsection{\zp / Las Vegas algorithms}
\zp contains languages $L$ such that there is a randomized algorithm $A$
that always outputs the correct answer in \textbf{expected polynomial time}.  
These are also called as Las Vegas algorithms.



