\documentclass[11pt]{book}
%\documentclass[10pt]{llncs}
%\usepackage{llncsdoc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listing}
\evensidemargin=0.20in
\oddsidemargin=0.20in
\topmargin=0.2in
%\headheight=0.0in
%\headsep=0.0in
%\setlength{\parskip}{0mm}
%\setlength{\parindent}{4mm}
\setlength{\textwidth}{6.4in}
\setlength{\textheight}{8.5in}
%\leftmargin -2in
%\setlength{\rightmargin}{-2in}
%\usepackage{epsf}
%\usepackage{url}

\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{enumitem}
\usepackage[euler-digits,small]{eulervm}
\usepackage{minted}
%\newminted{fortran}{fontsize=\footnotesize}
\newenvironment{code}{\VerbatimEnvironment \begin{minted}{haskell}}{\end{minted}}
% \newenvironment{code}{\begin{minted}{haskell}}{\end{minted}}

\usepackage{xargs}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage{epsfig}
\usepackage{tabularx}
\usepackage{latexsym}
\newcommand{\St}{\ensuremath{\mathcal{S}} }
\newcommand{\Act}{\ensuremath{\mathcal{A}} }
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\pik}{{\pi_{k}}}
\newcommand{\piknext}{{\pi_{k+1}}}
\newcommand{\pieps}{{\pi_{\epsilon}}}

\newcommand{\Vn}{{V_{n}}}
\newcommand{\Vnnext}{{V_{n+1}}}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}

\def\qed{$\Box$}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{proof}{Proof}

\title{Topics in machine learning: Naresh Manwani}
\author{Siddharth Bhat}
\date{Monsoon 2019}

\begin{document}


\maketitle
\tableofcontents

\chapter{Policy iteration}
\begin{align*}
    \pi_{k+1}(s) = arg \max_{a \in A(s)} r(s, a) + \gamma \sum_{s} P(s'|s, a) v_{\pi_k}(s')
\end{align*}

\begin{theorem}
    The policy iteration algorithm generates a sequecnce of policies with
    non-decreasing state values. That is, ${V^\piknext} \geq V^\pik$, 
    $V^\pi \in \Rn$, is the vector of state values for state $\pi$
\end{theorem}
\begin{proof}
    $F^\pik$ is the bellman expectation operator (?)

    Since $V^\pik$ is a fixed point of $F^\pik$, 
    \begin{align*}
        V^\pik &= F^\pik (V^\pik) \leq F(V^\pik) \qquad \text{(upper bounded by max value)} \\
        F(V^\pik) &=  F^\piknext (V^\pik) \qquad \text{(By defn of policy improvement step)} \\
        V^\pik &\leq F^\piknext (V^\pik) \qquad \text{(eqn 1)} \\
        F^\piknext (V^\pik) &\leq {(F^\piknext)}^2 (V^\pik) \qquad \text{(Monotonicity of $F^\piknext$)} \\
        \forall t \geq 1, ~F^\piknext (V^\pik) &\leq {(F^\piknext)}^t (V^\pik) \qquad \text{(Monotonicity of $F^\piknext$)} \\
        F^\piknext (V^\pik) &\leq {(F^\piknext)}^t (V^\pik) \leq V^\piknext \qquad \text{(Contraction mapping, $V^\piknext$ is fixed point)}
        \\
        V^\pik &= F^\piknext (V^\pik) \leq V^{\piknext}
    \end{align*}
\end{proof}

For a set of actions \Act and a set of states \St, the total number of
policies is ${|\Act^\St|}$. The number of computations per iteration is
$O(|\St|^3)$. So the loose upper bound is be ${O(|\St|^3 \times |\Act^\St|)}$.

\section{Value iteration algorithm}

\begin{code}
let v n s = max [r s a + gamma * sum [(p s' s a) * v (n-1) s' | s' <- ss] | a <- as]
let vs = [v i | i <- [0..]]
-- | L infinity
let norm v v' = max [(v s - v' s) | s <- ss]
let out = head $ 
  dropWhile (\v v' -> norm (v' - v) < eps * (1 - gamma) / (2 * gamma)) $ 
  zip vs (tail vs)
let policy s = argmax as $ \a -> 
  r s a + gamma * sum [ (p s' s a) * out s' | s' <- ss]
\end{code}

\begin{theorem}
    For the series $V_n$ and the policy $\pi_\epsilon$ computed by the value
    iteration algorithm, then:

    $$ \forall \epsilon > 0, ~\exists n_0 \in \N, \forall n \geq n_0, \quad ||V_{n+1} - V_n||_{\infty} \leq \frac{\epsilon (1 - \gamma)}{2 \gamma} $$
\end{theorem}
\begin{proof}
    We need to show that the sequence ${\{V_n\}_{n=0}^\infty}$ is a Cauchy sequence.
    This has ben proven before by the use of contraction mapping. Thus, for a
    given ${\epsilon' \geq 0, \exists n_0 \in \N, \forall n \geq n_0, ||V_{n+1} - V_n||_{\infty} \leq \epsilon'}$
    by cauchy sequence. So, pick ${\epsilon' = \frac{\epsilon(1 - \gamma)}{2 \gamma}}$, and the proof
    immediately follows.
\end{proof}

\begin{theorem}
    If $ ||V_{n+1} - V_n||_{\infty} \leq \frac{\epsilon (1 - \gamma)}{2 \gamma} $, then $||V_{n+1} - V^\star||_{\infty} < \epsilon / 2$
\end{theorem}
\begin{proof}
    \begin{align*}
        &||V_{n+1} - V^\star|| = 
        ||V_{n+1} -FV_{n+1} + F\Vnnext - V^\star|| \leq 
        || \Vnnext - FV^\star || + ||F\Vn - \Vn|| \qquad \text{(triangle inequality)} \\
        &\leq || \Vnnext - FV^\star ||  + \gamma || \Vnnext - V^\star || \\
        &\leq \gamma || \Vnnext - \Vn || + \gamma || \Vnnext - V^\star || \\
        &(1 - \gamma) || \Vnnext - V^\star || \leq \gamma || \Vn - \Vnnext || \qquad \text{(how?)} \\
        &\implies \dots
    \end{align*}
\end{proof}

It appears that $V^\pieps$ is just $V_{n+2}$??

\begin{theorem}
The policy $\pieps$ is $\epsilon$-optimal: $|| V^\star - V^\pieps|| \leq \epsilon$
\end{theorem}
% \begin{proof}
%     \begin{align*}
%         || V^\star - V^\pieps|| = || V^\star - \Vnnext + \Vnnext - V^\pieps|| \leq
%         || V^\star - \Vnnext || + || \Vnnext - V^\pieps|| \leq
%         \eps/2 + || \Vnnext - F \Vnnext + F \Vnnext - V^\pieps || 
%         % how did we replace V^\pieps with F^{\pieps} V^{\pieps}?
%         \leq \eps/2 + || \F \Vn - F \Vnnext + F \Vnnext - F^{\pieps} V^\pieps || 
%         \text{TODO: finish}
%     \end{align*}
% \end{proof}


\chapter{Monte carlo methods for MDP}

For dynamic programming, we needed to know the transition probability 
distribution $P(s, a, s')$, nor the reward function $r(s, a)$.

In the monte carlo methods, we assume that we do not know the transition 
probability distribution. We rely only on simulations.

This samples over \emph{episodes} for a fixed policy: sequences of states,
actions, and rewards.


\section{Naive}
\begin{itemize}
    \item For each $s \in S$, run $\pi$ from $s$ for $m$ times, where the $i$th episode is $T_i$.
    \item Let $r_i$ be the return of $T_i$
    \item Estimate the value of $\pi$ starting from $s$ as 
        ${\hat v_{\pi}(s) = \frac{1}{m} \sum_{i=1}^m r_i}$.
    \item Show by chernoff bounds that this is an OK estimate. We can use 
        Chernoff as ${ \{ r_i \} }$ are independent, 
        since the ${ \{ T_i \} }$ are independent.
\end{itemize}
\end{document}
