\documentclass[11pt]{book}
%\documentclass[10pt]{llncs}
%\usepackage{llncsdoc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listing}
\evensidemargin=0.20in
\oddsidemargin=0.20in
\topmargin=0.2in
%\headheight=0.0in
%\headsep=0.0in
%\setlength{\parskip}{0mm}
%\setlength{\parindent}{4mm}
\setlength{\textwidth}{6.4in}
\setlength{\textheight}{8.5in}
%\leftmargin -2in
%\setlength{\rightmargin}{-2in}
%\usepackage{epsf}
%\usepackage{url}

\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{enumitem}
\usepackage[euler-digits,small]{eulervm}
\usepackage{minted}
%\newminted{fortran}{fontsize=\footnotesize}
\newenvironment{code}{\VerbatimEnvironment \begin{minted}{haskell}}{\end{minted}}
% \newenvironment{code}{\begin{minted}{haskell}}{\end{minted}}

\usepackage{xargs}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage{epsfig}
\usepackage{tabularx}
\usepackage{latexsym}
\newcommand{\St}{\ensuremath{\mathcal{S}} }
\newcommand{\Act}{\ensuremath{\mathcal{A}} }
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\pik}{{\pi_{k}}}
\newcommand{\piknext}{{\pi_{k+1}}}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}

\def\qed{$\Box$}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{proof}{Proof}

\title{Topics in machine learning: Naresh Manwani}
\author{Siddharth Bhat}
\date{Monsoon 2019}

\begin{document}


\maketitle
\tableofcontents

\chapter{Policy iteration}
\begin{align*}
    \pi_{k+1}(s) = arg \max_{a \in A(s)} r(s, a) + \gamma \sum_{s} P(s'|s, a) v_{\pi_k}(s')
\end{align*}

\begin{theorem}
    The policy iteration algorithm generates a sequecnce of policies with
    non-decreasing state values. That is, ${V^\piknext} \geq V^\pik$, 
    $V^\pi \in \Rn$, is the vector of state values for state $\pi$
\end{theorem}
\begin{proof}
    $F^\pik$ is the bellman expectation operator (?)

    Since $V^\pik$ is a fixed point of $F^\pik$, 
    \begin{align*}
        V^\pik &= F^\pik (V^\pik) \leq F(V^\pik) \qquad \text{(upper bounded by max value)} \\
        F(V^\pik) &=  F^\piknext (V^\pik) \qquad \text{(By defn of policy improvement step)} \\
        V^\pik &\leq F^\piknext (V^\pik) \qquad \text{(eqn 1)} \\
        F^\piknext (V^\pik) &\leq {(F^\piknext)}^2 (V^\pik) \qquad \text{(Monotonicity of $F^\piknext$)} \\
        \forall t \geq 1, ~F^\piknext (V^\pik) &\leq {(F^\piknext)}^t (V^\pik) \qquad \text{(Monotonicity of $F^\piknext$)} \\
        F^\piknext (V^\pik) &\leq {(F^\piknext)}^t (V^\pik) \leq V^\piknext \qquad \text{(Contraction mapping, $V^\piknext$ is fixed point)}
        \\
        V^\pik &= F^\piknext (V^\pik) \leq V^{\piknext}
    \end{align*}
\end{proof}

For a set of actions \Act and a set of states \St, the total number of
policies is ${|\Act^\St|}$. The number of computations per iteration is
$O(|\St|^3)$. So the loose upper bound is be ${O(|\St|^3 \times |\Act^\St|)}$.

\section{Value iteration algorithm}

\begin{code}
let v n s = max [r s a + gamma * sum [(p s' s a) * v (n-1) s' | s' <- ss] | a <- as]
let vs = [v i | i <- [0..]]
let norm v v' = max [(v s - v' s) | s <- ss]
let out = head $ 
  dropWhile (\v v' -> norm (v' - v) < eps * (1 - gamma) / (2 * gamma)) $ 
  zip vs (tail vs)
let policy s = argmax as $ \a -> 
  r s a + gamma * sum [ (p s' s a) * out s' | s' <- ss]
\end{code}

\end{document}
